{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7843bf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 15:48:49 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/03/19 15:48:53 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.delta.DeltaAnalysisException",
     "evalue": " Cannot write to already existent path s3a://warehouse/delta/test without setting OVERWRITE = 'true'.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.delta.DeltaAnalysisException: Cannot write to already existent path s3a://warehouse/delta/test without setting OVERWRITE = 'true'.",
      "  at org.apache.spark.sql.delta.DeltaErrorsBase.pathAlreadyExistsException(DeltaErrors.scala:741)",
      "  at org.apache.spark.sql.delta.DeltaErrorsBase.pathAlreadyExistsException$(DeltaErrors.scala:740)",
      "  at org.apache.spark.sql.delta.DeltaErrors$.pathAlreadyExistsException(DeltaErrors.scala:2489)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:193)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:98)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:91)",
      "  at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:252)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:91)",
      "  at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:159)",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:357)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.range(5).write.format(\"delta\").save(\"s3a://warehouse/delta/test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc5108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c8570a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://909a8a93cede:4042\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1679242832888)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 16:20:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/03/19 16:20:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/03/19 16:20:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/03/19 16:20:44 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/03/19 16:20:44 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.160.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.catalyst.catalog.ExternalCatalog = org.apache.spark.sql.hive.HiveExternalCatalog@200ea6ea\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sharedState.externalCatalog.unwrapped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaffc22-fc3c-4fb0-9aaa-18778e7b1ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdf2f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.app.id,local-1679238407724)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.hadoop.fs.s3a.access.key,admin)\n",
      "(spark.driver.extraJavaOptions,-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED)\n",
      "(spark.hadoop.fs.s3a.path.style.access,true)\n",
      "(spark.hadoop.javax.jdo.option.ConnectionDriverName,org.postgresql.Driver)\n",
      "(spark.hadoop.fs.s3a.secret.key,password)\n",
      "(spark.app.name,spylon-kernel)\n",
      "(spark.history.fs.logDirectory,/home/iceberg/spark-events)\n",
      "(spark.eventLog.dir,/home/iceberg/spark-events)\n",
      "(spark.sql.catalog.iceberg.catalog-impl,org.apache.iceberg.rest.RESTCatalog)\n",
      "(spark.sql.catalog.iceberg.s3.endpoint,http://minio:9000)\n",
      "(spark.repl.class.outputDir,/tmp/tmpzt8w65l3)\n",
      "(spark.serializer.objectStreamReset,100)\n",
      "(spark.sql.defaultCatalog,iceberg)\n",
      "(spark.master,local[*])\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.app.startTime,1679238407030)\n",
      "(spark.hadoop.javax.jdo.option.ConnectionURL,jdbc:postgresql://metastore:5432/metastore)\n",
      "(spark.sql.warehouse.dir,s3a://warehouse/delta/)\n",
      "(spark.hadoop.javax.jdo.option.ConnectionPassword,admin)\n",
      "(spark.app.submitTime,1679238406812)\n",
      "(spark.sql.catalog.iceberg.io-impl,org.apache.iceberg.aws.s3.S3FileIO)\n",
      "(spark.executor.id,driver)\n",
      "(spark.sql.catalog.iceberg.warehouse,s3a://warehouse/wh/)\n",
      "(spark.repl.class.uri,spark://86d9e0b7a461:42523/classes)\n",
      "(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)\n",
      "(spark.sql.catalog.iceberg,org.apache.iceberg.spark.SparkCatalog)\n",
      "(spark.sql.catalogImplementation,hive)\n",
      "(spark.sql.catalog.iceberg.uri,http://rest:8181)\n",
      "(spark.rdd.compress,True)\n",
      "(spark.executor.extraJavaOptions,-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED)\n",
      "(spark.driver.host,86d9e0b7a461)\n",
      "(spark.submit.pyFiles,)\n",
      "(spark.sql.extensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension)\n",
      "(spark.hadoop.fs.s3a.endpoint,http://minio:9000)\n",
      "(spark.ui.showConsoleProgress,true)\n",
      "(spark.hadoop.javax.jdo.option.ConnectionUserName,admin)\n",
      "(spark.driver.port,42523)\n",
      "(spark.sql.catalog.spark_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog)"
     ]
    }
   ],
   "source": [
    "print(sc.getConf.getAll.mkString(\"\\n\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1dcebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " DELETE is only supported with v2 tables.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: DELETE is only supported with v2 tables.",
      "  at org.apache.spark.sql.errors.QueryCompilationErrors$.deleteOnlySupportedWithV2TablesError(QueryCompilationErrors.scala:812)",
      "  at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:288)",
      "  at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)",
      "  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)",
      "  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)",
      "  at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)",
      "  at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)",
      "  at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)",
      "  at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)",
      "  at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)",
      "  at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)",
      "  at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)",
      "  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)",
      "  at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)",
      "  at org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)",
      "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)",
      "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)",
      "  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)",
      "  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:106)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"delete from spark_catalog.default.my_orc where id = 1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a051fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/30 09:18:16 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " SHOW CREATE TABLE doesn't support transactional Hive table. Please use `SHOW CREATE TABLE `default`.`my_orc` AS SERDE` to show Hive DDL instead.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: SHOW CREATE TABLE doesn't support transactional Hive table. Please use `SHOW CREATE TABLE `default`.`my_orc` AS SERDE` to show Hive DDL instead.",
      "  at org.apache.spark.sql.errors.QueryCompilationErrors$.showCreateTableNotSupportTransactionalHiveTableError(QueryCompilationErrors.scala:1981)",
      "  at org.apache.spark.sql.execution.command.ShowCreateTableCommand.run(tables.scala:1118)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table spark_catalog.default.my_orc\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
